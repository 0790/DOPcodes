{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wu4uEcJWCoku"
   },
   "outputs": [],
   "source": [
    "#implementing feed forward spiking neural network as described in Surrogate Gradient Learning in Spiking Neural Networks. \n",
    "#https://arxiv.org/abs/1901.09948\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A9C_m33BEaYu"
   },
   "outputs": [],
   "source": [
    "#using LIF neurons, setting hyperparameters.\n",
    "#number of neurons in each layer N = 128\n",
    "#learning rate alpha = 0.001\n",
    "#betas for Adam optimizer, first and second momemt = 0.9, 0.999\n",
    "#batch size for minibatch Nbatch = 256\n",
    "#threshold potential Uthres = 1\n",
    "#reset potential Urest = 0\n",
    "#synaptic time constant tausyn = 10 ms = 0.01 s\n",
    "#membrane time constant taumem = 20 ms = 0.02 s\n",
    "#let the exponential form of time constants be lambd and muh\n",
    "#refractory time constant tref = 0 ms\n",
    "#time step size = 0.5 ms = 0.0005 s\n",
    "#total simulation duration = 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HUETpaTcH3ys"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from utils import get_shd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K8S_QW5A4ftU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://compneuro.net/datasets/shd_train.h5.gz\n",
      "File shd_train.h5.gz decompressed to:\n",
      "/tmp\\.data-cache\\hdspikes\\shd_train.h5.gz\n",
      "Downloading data from https://compneuro.net/datasets/shd_test.h5.gz\n",
      "File shd_test.h5.gz decompressed to:\n",
      "/tmp\\.data-cache\\hdspikes\\shd_test.h5.gz\n",
      "C:\\Users\\Deepti\\Desktop\\DOPcodes\\SHD\\Original_jupyter_notebooks\\dataset\\shddataset\\\n"
     ]
    }
   ],
   "source": [
    "cache_dir = os.path.expanduser(\"~/data\")\n",
    "cache_subdir = \"hdspikes\"\n",
    "get_shd_dataset(cache_dir, cache_subdir)\n",
    "#train_file = h5py.File('/content/drive/MyDrive/dataset/shd_train.h5','r')\n",
    "#test_file = h5py.File('/content/drive/MyDrive/dataset/shd_test.h5','r')\n",
    "\n",
    "#cache_dir = os.path.expanduser(\"~/data\")\n",
    "#cache_subdir = \"hdspikes\"\n",
    "basepath = os.getcwd() \n",
    "\n",
    "direc = \"\\dataset\\shddataset\\\\\"\n",
    "print(basepath+direc)\n",
    "#file_path = get_shd_dataset(cache_dir = basepath, cache_subdir =direc)\n",
    "\n",
    "#directrain = \"~/dataset/shddataset/shd_train.h5\"\n",
    "#directest = \"~/dataset/shddataset/shd_test.h5\"\n",
    "#print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = h5py.File((basepath + direc +  \"shd_train.h5\"), 'r')\n",
    "test_file = h5py.File((basepath +direc + \"shd_test.h5\"), 'r')\n",
    "\n",
    "\n",
    "x_train = train_file['spikes']\n",
    "y_train = train_file['labels']\n",
    "x_test = test_file['spikes']\n",
    "y_test = test_file['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "oG3tRijHS-9y",
    "outputId": "87661920-fdeb-4407-fb75-78efd10ffcb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"import tables\\n#fileh = tables.open_file(\\'/content/drive/MyDrive/dataset/shd_train.h5\\', mode=\\'r\\')\\nfileh = tables.open_file(os.path.join(cache_dir, cache_subdir, \\'shd_train.h5\\'),mode = \\'r\\')\\nunits = fileh.root.spikes.units\\ntimes = fileh.root.spikes.times\\nlabels = fileh.root.labels\\nindex = 0\\nprint(\"Times (ms):\", times[index])\\nprint(\"Unit IDs:\", units[index])\\nprint(\"Label:\", labels[index])\\nimport matplotlib.pyplot as plt\\n \\nfig = plt.figure(figsize=(16,4))\\nidx = np.random.randint(len(times),size=5)\\nfor i,k in enumerate(idx):\\n    ax = plt.subplot(1,5,i+1)\\n    ax.scatter(times[k],700-units[k], color=\"k\", alpha=0.33, s=2)\\n    ax.set_title(\"Label %i\"%labels[k])\\n    ax.axis(\"off\")\\n \\nplt.show()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"import tables\n",
    "#fileh = tables.open_file('/content/drive/MyDrive/dataset/shd_train.h5', mode='r')\n",
    "fileh = tables.open_file(os.path.join(cache_dir, cache_subdir, 'shd_train.h5'),mode = 'r')\n",
    "units = fileh.root.spikes.units\n",
    "times = fileh.root.spikes.times\n",
    "labels = fileh.root.labels\n",
    "index = 0\n",
    "print(\"Times (ms):\", times[index])\n",
    "print(\"Unit IDs:\", units[index])\n",
    "print(\"Label:\", labels[index])\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "fig = plt.figure(figsize=(16,4))\n",
    "idx = np.random.randint(len(times),size=5)\n",
    "for i,k in enumerate(idx):\n",
    "    ax = plt.subplot(1,5,i+1)\n",
    "    ax.scatter(times[k],700-units[k], color=\"k\", alpha=0.33, s=2)\n",
    "    ax.set_title(\"Label %i\"%labels[k])\n",
    "    ax.axis(\"off\")\n",
    " \n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Mln39Xz_H944"
   },
   "outputs": [],
   "source": [
    "N = 128 #per layer\n",
    "Nin = 700 #input from 700 bushy cells\n",
    "Nout = 20 #0-9 in english and german\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "Nbatch = 256\n",
    "\n",
    "\n",
    "Uthres = 1\n",
    "Urest = 0\n",
    "tausyn = 0.01 \n",
    "taumem =  0.02 \n",
    "tref = 0\n",
    "steep = 100\n",
    "\n",
    "t= 0.0005 \n",
    "T = 1.5\n",
    "Ntimesteps = int(T/t)\n",
    "\n",
    "lambd = float(np.exp(-t/tausyn))\n",
    "muh = float(np.exp(-t/taumem))\n",
    "\n",
    "datatype = torch.float\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LuYVt45F6aL",
    "outputId": "392d55d7-1f1e-45d0-81cb-8f7bacc4e62e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['times', 'units']>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xx5m_bVXREVX",
    "outputId": "eb3de0d7-3771-4f90-9312-8338e449e7ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([0.      , 0.001833, 0.002167, ..., 0.698   , 0.6997  , 0.7007  ],\n",
       "             dtype=float16)                                                    ,\n",
       "       array([0.000e+00, 5.002e-04, 5.165e-03, ..., 8.789e-01, 8.794e-01,\n",
       "              8.809e-01], dtype=float16)                                 ,\n",
       "       array([0.000e+00, 4.168e-04, 2.041e-03, ..., 7.378e-01, 7.383e-01,\n",
       "              7.393e-01], dtype=float16)                                 ,\n",
       "       ...,\n",
       "       array([0.000e+00, 7.501e-04, 1.166e-03, ..., 8.174e-01, 8.242e-01,\n",
       "              8.262e-01], dtype=float16)                                 ,\n",
       "       array([0.      , 0.001083, 0.001875, ..., 0.631   , 0.6313  , 0.6313  ],\n",
       "             dtype=float16)                                                    ,\n",
       "       array([0.       , 0.0005417, 0.0006666, ..., 0.522    , 0.5225   ,\n",
       "              0.5234   ], dtype=float16)                                 ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[\"times\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gROCZVk46xIQ",
    "outputId": "5c9a03a4-3ecb-4392-d6db-1d2d9b11ef22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized weights\n"
     ]
    }
   ],
   "source": [
    "#size of weight matrix : w1 = (700,128) w2 = (128,20)\n",
    "#https://pytorch.org/docs/stable/generated/torch.empty.html\n",
    "w1 = torch.empty((Nin,N), dtype = datatype , device = device , requires_grad= True) #require_grad = true to record the operations on the weight matrix\n",
    "w2 = torch.empty((N,Nout), dtype = datatype , device = device , requires_grad= True) #transpose of this is multiplied with incoming spikes\n",
    "\n",
    "#normalize using He initialization\n",
    "#https://pytorch.org/docs/stable/nn.init.html\n",
    "torch.nn.init.uniform_(w1, a=-np.sqrt(2.0/Nin), b=np.sqrt(2.0/Nin)) #given as uniform distribution in papers, but normal distribution in spytorch code\n",
    "torch.nn.init.uniform_(w2, a=-np.sqrt(2.0/N), b=np.sqrt(2.0/N))\n",
    "print(\"initialized weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RPJ6-3bHZs7s"
   },
   "outputs": [],
   "source": [
    "import torch.autograd as auto \n",
    "#https://pytorch.org/docs/stable/autograd.html\n",
    "#https://snntorch.readthedocs.io/en/latest/snntorch.surrogate.html\n",
    "class SurGrad(auto.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = torch.zeros_like(i) #i here is the membrane potential over the threshold incoming to the current layer\n",
    "        result[i>Uthres] = 1.0\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "  \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        result, = ctx.saved_tensors #U is stored in results\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(steep*torch.abs(torch.as_tensor(result,device = device)) +1.0 )**2\n",
    "        return grad\n",
    "\n",
    "spikefunction = SurGrad.apply \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3DEIxZhaXbUN"
   },
   "outputs": [],
   "source": [
    "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
    "    \"\"\" This generator takes a spike dataset and generates spiking network input as sparse tensors. \n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    #labels_ = np.array(y,dtype=np.int) \n",
    "    labels_ = np.array(y, dtype = int )\n",
    "    number_of_batches = len(labels_)//batch_size\n",
    "    sample_index = np.arange(len(labels_))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    firing_times = X['times']\n",
    "    units_fired = X['units']\n",
    "    \n",
    "    time_bins = np.linspace(0, max_time, num=nb_steps)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            times = np.digitize(firing_times[idx], time_bins)\n",
    "            units = units_fired[idx]\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "            \n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "    \n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "N9geAI6hjYaO"
   },
   "outputs": [],
   "source": [
    "#forward pass for the SNN\n",
    "def forwarddynamic(input):\n",
    "    Isyn1 = torch.zeros((Nbatch,N), dtype= datatype , device = device)\n",
    "    Umem1 = torch.zeros((Nbatch,N), dtype= datatype , device = device)\n",
    "    #membrane potential and synaptic current will have same dimensions as the weights.T in a mini batch  gradient descent\n",
    "    Isynnext1 = torch.zeros((Nbatch,N), dtype= datatype , device = device)\n",
    "    Umemnext1 = torch.zeros((Nbatch,N), dtype= datatype , device = device)\n",
    "\n",
    "    Urecord1 = []\n",
    "    Spikerecord = []\n",
    "\n",
    "\n",
    "  #hidden layer 1\n",
    "    outputlayer1 = torch.zeros((Nbatch,N), dtype= datatype , device = device)\n",
    "    z1 = torch.einsum(\"ijk,kl->ijl\" , (input, w1))   \n",
    "    \"\"\"i gives the spike train number, j and k describe the firing time and the neuron unit which fires. k and l describes the dimesnions of weight matrix. k is\n",
    "      the index of multiplication ==> dot product\"\"\"\n",
    "    for t in range(Ntimesteps):\n",
    "        outputlayer1 = spikefunction(Umem1)\n",
    "        resetspike = outputlayer1.detach() #what does this do?\n",
    "\n",
    "        Isynnext1 = lambd*Isyn1 + z1[:,t] # I[t+1] = lambda * I[t] + w1*inputspike  (no V matrix for feedforward)\n",
    "        Umemnext1 = (muh*Umem1)*(1-resetspike) +Isyn1  #according to SHD paper, not Surgrad notebook\n",
    "\n",
    "        Umem1 = Umemnext1\n",
    "        Isyn1 = Isynnext1\n",
    "        Spikerecord.append(outputlayer1)\n",
    "        Urecord1.append(Umem1)\n",
    "\n",
    "\n",
    "    Urecord1 = torch.stack(Urecord1,dim=1)\n",
    "    Spikerecord = torch.stack(Spikerecord, dim=1)\n",
    "\n",
    "  #hidden unit to output layer, output layer is leaky integrator which don't spike \n",
    "  #purpose of this layer is to record the membrane potential of the last layer\n",
    "\n",
    "    z2 = torch.einsum(\"ijk,kl->ijl\" , (Spikerecord , w2) )\n",
    "    Isyn2 = torch.zeros((Nbatch,Nout), dtype= datatype , device = device)\n",
    "    Umem2 = torch.zeros((Nbatch,Nout), dtype= datatype , device = device) \n",
    "\n",
    "    output_potential = [Umem2]\n",
    "    for t in range(Ntimesteps):\n",
    "        Isynnext2 = lambd*Isyn2 + z2[:,t]\n",
    "        Umemnext2 = muh* Umem2 + Isyn2\n",
    "\n",
    "        Isyn2 = Isynnext2\n",
    "        Umem2 = Umemnext2\n",
    "\n",
    "        output_potential.append(Umem2)\n",
    "\n",
    "    output_potential = torch.stack(output_potential,dim=1)\n",
    "    records = [Urecord1 , Spikerecord]\n",
    "    return output_potential , records\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "t9N3Z1GJSa0-"
   },
   "outputs": [],
   "source": [
    "def trainning(x , y , alpha= alpha , Nepochs = 10):\n",
    "    parameters = [w1,w2]\n",
    "    #using softmax function for negative likelihood loss calculation\n",
    "    loss_record = []\n",
    "    for i in range(Nepochs):\n",
    "        print(\"Epoch number: %i\" %(i+1) )\n",
    "        local_loss = []\n",
    "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(X = x,y = y,batch_size= Nbatch, nb_steps= Ntimesteps,nb_units= Nin,max_time= T):\n",
    "            #print(\"Batch processing\" )\n",
    "            output,records = forwarddynamic(x_local.to_dense())\n",
    "            _,spikes = records #this is used in regularization\n",
    "            #https://pytorch.org/docs/stable/generated/torch.max.html\n",
    "            maximum,_ = torch.max(output,dim =1)  #calculates maximum membrane potential in the entire duration\n",
    "            logsoftmax = nn.LogSoftmax(dim=1)\n",
    "            y_network =logsoftmax(maximum)\n",
    "            #strength of the spike limit is given by 2 x 10^ -6\n",
    "            reg = 2e-6*torch.sum(spikes) + 2e-6*torch.mean(torch.sum(torch.sum(spikes,dim=0),dim=0)**2)\n",
    "            if device == 'cpu':\n",
    "                y_local = y_local.type(torch.LongTensor)\n",
    "            elif device == 'cuda':\n",
    "                y_local = y_local.type(torch.cuda.LongTensor)\n",
    "            y_local = y_local.type(torch.LongTensor)\n",
    "            #https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "            loss = nn.NLLLoss()(y_network , y_local)\n",
    "            loss = loss + reg\n",
    "\n",
    "            optim = torch.optim.Adamax(parameters, lr = alpha, betas = (beta1,beta2))\n",
    "            optim.zero_grad()   #sets the gradient of optimised weights to 0. https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            local_loss.append(loss.item())\n",
    "    mean_loss = np.mean(local_loss)\n",
    "    loss_record.append(mean_loss)\n",
    "    print(\"Epoch %i: loss= %.5f\"%(i+1,mean_loss))\n",
    "    return loss_record\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "13pwshCquqqk"
   },
   "outputs": [],
   "source": [
    "def accuracy(x,y):\n",
    "    acc = []\n",
    "    for x_local, y_local in sparse_data_generator_from_hdf5_spikes(X =x,y= y,batch_size = Nbatch, nb_steps = Ntimesteps, nb_units = Nin, max_time = T,shuffle=False):\n",
    "        output,_ = forwarddynamic(x_local.to_dense())\n",
    "        maximum,_ = torch.max(output,dim =1)  #calculates maximum membrane potential in the entire duration\n",
    "        _,y_unit = torch.max(maximum,1) #the unit which had maximum\n",
    "        tmp = np.mean((y_local==y_unit).detach().cpu().numpy()) # compare to labels\n",
    "        acc.append(tmp)\n",
    "    return np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYVT4bBb0kWA",
    "outputId": "8aa56401-0fb4-4543-ffdd-3b7be9b828d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1\n",
      "Batch processing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEEPTI~1\\AppData\\Local\\Temp/ipykernel_27472/741897780.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\DEEPTI~1\\AppData\\Local\\Temp/ipykernel_27472/3267886780.py\u001b[0m in \u001b[0;36mtrainning\u001b[1;34m(x, y, alpha, Nepochs)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx_local\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_local\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msparse_data_generator_from_hdf5_spikes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mNbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mNtimesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb_units\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mNin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_time\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Batch processing\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforwarddynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspikes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecords\u001b[0m \u001b[1;31m#this is used in regularization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;31m#https://pytorch.org/docs/stable/generated/torch.max.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_hist = trainning(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBZW9oqDdfHG"
   },
   "outputs": [],
   "source": [
    "def plotloss(loss_hist):\n",
    "    plt.figure()\n",
    "    plt.plot(loss_hist, label=\"Loss\") \n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-LS1FWNnuF3"
   },
   "outputs": [],
   "source": [
    "plotloss(loss_hist)\n",
    "print(\"Trained for 10 epochs\")\n",
    "print(\"Training accuracy: %.3f\"%(accuracy(x_train,y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JSBalH3b8S3"
   },
   "outputs": [],
   "source": [
    "Nepochs = 250\n",
    "loss_hist = trainning(x_train, y_train , Nepochs = Nepochs)\n",
    "plotloss(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQOVvrI2b6tO"
   },
   "outputs": [],
   "source": [
    "print(\"Training accuracy: %.3f\"%(accuracy(x_train,y_train)))\n",
    "print(\"Test accuracy: %.3f\"%(accuracy(x_test,y_test)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TestFeedForwardSNN-SHD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
