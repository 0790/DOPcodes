{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5rhdLt8prJHR"
   },
   "outputs": [],
   "source": [
    "#implementing recurrent spiking neural network as described in Surrogate Gradient Learning in Spiking Neural Networks. https://arxiv.org/abs/1901.09948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zc-gHloBrJHT"
   },
   "outputs": [],
   "source": [
    "#using LIF neurons, setting hyperparameters.\n",
    "#number of neurons in each layer N = 128\n",
    "#learning rate alpha = 0.001\n",
    "#betas for Adam optimizer, first and second momemt = 0.9, 0.999\n",
    "#batch size for minibatch Nbatch = 256\n",
    "#threshold potential Uthres = 1\n",
    "#reset potential Urest = 0\n",
    "#synaptic time constant tausyn = 10 ms = 0.01 s\n",
    "#membrane time constant taumem = 20 ms = 0.02 s\n",
    "#let the exponential form of time constants be lambd and muh\n",
    "#refractory time constant tref = 0 ms\n",
    "#time step size = 0.5 ms = 0.0005 s\n",
    "#total simulation duration = 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ild4AJ3WrJHV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deepti Kumar\\Desktop\\sopcodes\\dataset\\shddataset\\\n"
     ]
    }
   ],
   "source": [
    "basepath = os.getcwd() \n",
    "\n",
    "direc = \"\\dataset\\shddataset\\\\\"\n",
    "print(basepath+direc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5nXkT69hrJHW"
   },
   "outputs": [],
   "source": [
    "train_file = h5py.File((basepath + direc +  \"shd_train.h5\"), 'r')\n",
    "test_file = h5py.File((basepath +direc + \"shd_test.h5\"), 'r')\n",
    "\n",
    "x_train = train_file['spikes']\n",
    "y_train = train_file['labels']\n",
    "x_test = test_file['spikes']\n",
    "y_test = test_file['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "tWEePjatrJHX",
    "outputId": "c23f1ba4-75ee-4e38-b432-bc37c109f1aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import tables\\nfileh = tables.open_file(\\'/content/drive/MyDrive/dataset/shd_train.h5\\', mode=\\'r\\')\\nunits = fileh.root.spikes.units\\ntimes = fileh.root.spikes.times\\nlabels = fileh.root.labels\\nindex = 0\\nprint(\"Times (ms):\", times[index])\\nprint(\"Unit IDs:\", units[index])\\nprint(\"Label:\", labels[index])\\nimport matplotlib.pyplot as plt\\n \\nfig = plt.figure(figsize=(16,4))\\nidx = np.random.randint(len(times),size=5)\\nfor i,k in enumerate(idx):\\n    ax = plt.subplot(1,5,i+1)\\n    ax.scatter(times[k],700-units[k], color=\"k\", alpha=0.33, s=2)\\n    ax.set_title(\"Label %i\"%labels[k])\\n    ax.axis(\"off\")\\n \\nplt.show()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import tables\n",
    "fileh = tables.open_file('/content/drive/MyDrive/dataset/shd_train.h5', mode='r')\n",
    "units = fileh.root.spikes.units\n",
    "times = fileh.root.spikes.times\n",
    "labels = fileh.root.labels\n",
    "index = 0\n",
    "print(\"Times (ms):\", times[index])\n",
    "print(\"Unit IDs:\", units[index])\n",
    "print(\"Label:\", labels[index])\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "fig = plt.figure(figsize=(16,4))\n",
    "idx = np.random.randint(len(times),size=5)\n",
    "for i,k in enumerate(idx):\n",
    "    ax = plt.subplot(1,5,i+1)\n",
    "    ax.scatter(times[k],700-units[k], color=\"k\", alpha=0.33, s=2)\n",
    "    ax.set_title(\"Label %i\"%labels[k])\n",
    "    ax.axis(\"off\")\n",
    " \n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YTHMeGpSrJHY"
   },
   "outputs": [],
   "source": [
    "N = 128 #per layer\n",
    "Nin = 700 #input from 700 bushy cells\n",
    "Nout = 20 #0-9 in english and german\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "Nbatch = 256\n",
    "\n",
    "\n",
    "Uthres = 1\n",
    "Urest = 0\n",
    "tausyn = 0.01 \n",
    "taumem =  0.02 \n",
    "tref = 0\n",
    "steep = 100\n",
    "\n",
    "t= 0.0005 \n",
    "T = 1.5\n",
    "Ntimesteps = int(T/t)\n",
    "\n",
    "lambd = float(np.exp(-t/tausyn))\n",
    "muh = float(np.exp(-t/taumem))\n",
    "\n",
    "datatype = torch.float\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEzb2qqnrJHZ",
    "outputId": "d09a756c-ac02-49c7-d43c-a06e0c8d4fff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['times', 'units']>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMFsVu_DrJHa",
    "outputId": "2fc2c1e5-1eae-4716-9fd9-3e691ca8caf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([0.      , 0.001833, 0.002167, ..., 0.698   , 0.6997  , 0.7007  ],\n",
       "             dtype=float16)                                                    ,\n",
       "       array([0.000e+00, 5.002e-04, 5.165e-03, ..., 8.789e-01, 8.794e-01,\n",
       "              8.809e-01], dtype=float16)                                 ,\n",
       "       array([0.000e+00, 4.168e-04, 2.041e-03, ..., 7.378e-01, 7.383e-01,\n",
       "              7.393e-01], dtype=float16)                                 ,\n",
       "       ...,\n",
       "       array([0.000e+00, 7.501e-04, 1.166e-03, ..., 8.174e-01, 8.242e-01,\n",
       "              8.262e-01], dtype=float16)                                 ,\n",
       "       array([0.      , 0.001083, 0.001875, ..., 0.631   , 0.6313  , 0.6313  ],\n",
       "             dtype=float16)                                                    ,\n",
       "       array([0.       , 0.0005417, 0.0006666, ..., 0.522    , 0.5225   ,\n",
       "              0.5234   ], dtype=float16)                                 ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[\"times\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmmTFH-RrJHa",
    "outputId": "5c60d77f-4134-4b64-83ec-fba8a335afbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized weights\n"
     ]
    }
   ],
   "source": [
    "#size of weight matrix : w1 = (700,128) w2 = (128,20)\n",
    "#https://pytorch.org/docs/stable/generated/torch.empty.html\n",
    "w1 = torch.empty((Nin,N), dtype = datatype , device = device , requires_grad= True) #require_grad = true to record the operations on the weight matrix\n",
    "w2 = torch.empty((N,Nout), dtype = datatype , device = device , requires_grad= True) #transpose of this is multiplied with incoming spikes\n",
    "v1 = torch.empty((N,N), dtype = datatype , device = device , requires_grad= True)\n",
    "#normalize using He initialization\n",
    "#https://pytorch.org/docs/stable/nn.init.html\n",
    "torch.nn.init.uniform_(w1, a=-np.sqrt(2.0/Nin), b=np.sqrt(2.0/Nin)) #given as uniform distribution in papers, but normal distribution in spytorch code\n",
    "torch.nn.init.uniform_(w2, a=-np.sqrt(2.0/N), b=np.sqrt(2.0/N))\n",
    "torch.nn.init.uniform_(v1, a =-np.sqrt(2.0/N), b= np.sqrt(2.0/N))\n",
    "print(\"initialized weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eWdb0_ltrJHb"
   },
   "outputs": [],
   "source": [
    "import torch.autograd as auto \n",
    "#https://pytorch.org/docs/stable/autograd.html\n",
    "#https://snntorch.readthedocs.io/en/latest/snntorch.surrogate.html\n",
    "class SurGrad(auto.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = torch.zeros_like(i) #i here is the membrane potential over the threshold incoming to the current layer\n",
    "        result[i>Uthres] = 1.0\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "  \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        result, = ctx.saved_tensors #U is stored in results\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(steep*torch.abs(torch.as_tensor(result,device = device)) +1.0 )**2\n",
    "        return grad\n",
    "\n",
    "spikefunction = SurGrad.apply \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NF84Ba0SrJHc"
   },
   "outputs": [],
   "source": [
    "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
    "    \"\"\" This generator takes a spike dataset and generates spiking network input as sparse tensors. \n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "    labels_ = np.array(y,dtype=int)\n",
    "    number_of_batches = len(labels_)//batch_size\n",
    "    sample_index = np.arange(len(labels_))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    firing_times = X['times']\n",
    "    units_fired = X['units']\n",
    "    \n",
    "    time_bins = np.linspace(0, max_time, num=nb_steps)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            times = np.digitize(firing_times[idx], time_bins)\n",
    "            units = units_fired[idx]\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "            \n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "    \n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "e9wt6iIDrJHd"
   },
   "outputs": [],
   "source": [
    "#forward pass for the SNN\n",
    "def forwarddynamic(input):\n",
    "    Isyn1 = torch.zeros((Nbatch,N), dtype= datatype , device = device)\n",
    "    Umem1 = torch.zeros((Nbatch,N), dtype= datatype , device = device)\n",
    "    #membrane potential and synaptic current will have same dimensions as the weights.T in a mini batch  gradient descent\n",
    "    Isynnext1 = torch.zeros((Nbatch,N), dtype= datatype , device = device)\n",
    "    Umemnext1 = torch.zeros((Nbatch,N), dtype= datatype , device = device)\n",
    "\n",
    "    Urecord1 = []\n",
    "    Spikerecord = []\n",
    "\n",
    "\n",
    "    #hidden layer 1\n",
    "    outputlayer1 = torch.zeros((Nbatch,N), dtype= datatype , device = device)\n",
    "    z1 = torch.einsum(\"ijk,kl->ijl\" , (input, w1))   \n",
    "    \"\"\"i gives the spike train number, j and k describe the firing time and the neuron unit which fires. k and l describes the dimesnions of weight matrix. k is\n",
    "    the index of multiplication ==> dot product\"\"\"\n",
    "    for t in range(Ntimesteps):\n",
    "        z1_recur =z1[:,t] +torch.einsum(\"jk,kl->jl\" , (outputlayer1, v1)) #the ouput of the current layer and the input spikes are\n",
    "                                                                     #used as the input for recurrent connections\n",
    "        outputlayer1 = spikefunction(Umem1)\n",
    "        resetspike = outputlayer1.detach() #what does this do?\n",
    "\n",
    "        Isynnext1 = lambd*Isyn1 + z1_recur # I[t+1] = lambda * I[t] + w1*inputspike  (no V matrix for feedforward)\n",
    "        Umemnext1 = (muh*Umem1)*(1-resetspike) +Isyn1  #according to SHD paper, not Surgrad notebook\n",
    "\n",
    "        Umem1 = Umemnext1\n",
    "        Isyn1 = Isynnext1\n",
    "        Spikerecord.append(outputlayer1)\n",
    "        Urecord1.append(Umem1)\n",
    "\n",
    "\n",
    "    Urecord1 = torch.stack(Urecord1,dim=1)\n",
    "    Spikerecord = torch.stack(Spikerecord, dim=1)\n",
    "\n",
    "  #hidden unit to output layer, output layer is leaky integrator which don't spike \n",
    "  #purpose of this layer is to record the membrane potential of the last layer\n",
    "\n",
    "    z2 = torch.einsum(\"ijk,kl->ijl\" , (Spikerecord , w2) )\n",
    "    Isyn2 = torch.zeros((Nbatch,Nout), dtype= datatype , device = device)\n",
    "    Umem2 = torch.zeros((Nbatch,Nout), dtype= datatype , device = device) \n",
    "\n",
    "    output_potential = [Umem2]\n",
    "    for t in range(Ntimesteps):\n",
    "        Isynnext2 = lambd*Isyn2 + z2[:,t]\n",
    "        Umemnext2 = muh* Umem2 + Isyn2\n",
    "\n",
    "        Isyn2 = Isynnext2\n",
    "        Umem2 = Umemnext2\n",
    "\n",
    "        output_potential.append(Umem2)\n",
    "\n",
    "    output_potential = torch.stack(output_potential,dim=1)\n",
    "    records = [Urecord1 , Spikerecord]\n",
    "    return output_potential , records\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vPGemC6CrJHf"
   },
   "outputs": [],
   "source": [
    "def training(x , y , alpha= alpha , Nepochs = 10):\n",
    "    parameters = [w1,w2]\n",
    "  \n",
    "    #using softmax function for negative likelihood loss calculation\n",
    "    loss_record = []\n",
    "    for i in range(Nepochs):\n",
    "        print(\"Epoch %i\"%(i+1))\n",
    "        local_loss = []\n",
    "        j = 0 \n",
    "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(X = x,y = y,batch_size= Nbatch, nb_steps= Ntimesteps,nb_units= Nin,max_time= T):\n",
    "            #print(\"Batch Number: %i\"%(j+1)) \n",
    "            j = j+1\n",
    "            output,records = forwarddynamic(x_local.to_dense())\n",
    "            _,spikes = records #this is used in regularization\n",
    "      #https://pytorch.org/docs/stable/generated/torch.max.html\n",
    "            maximum,_ = torch.max(output,dim =1)  #calculates maximum membrane potential in the entire duration\n",
    "            logsoftmax = nn.LogSoftmax(dim=1)\n",
    "            y_network =logsoftmax(maximum)\n",
    "\n",
    "      #strength of the spike limit is given by 2 x 10^ -6\n",
    "            reg = 2e-6*torch.sum(spikes) + 2e-6*torch.mean(torch.sum(torch.sum(spikes,dim=0),dim=0)**2)\n",
    "            if device == 'cpu':\n",
    "                y_local = y_local.type(torch.LongTensor)\n",
    "            elif device == 'cuda':\n",
    "                y_local = y_local.type(torch.cuda.LongTensor)\n",
    "      #https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "            loss = nn.NLLLoss()(y_network , y_local)\n",
    "            loss = loss + reg\n",
    "\n",
    "            optim = torch.optim.Adamax(parameters, lr = alpha, betas = (beta1,beta2))\n",
    "            optim.zero_grad() \n",
    "      #sets the gradient of optimised weights to 0. https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            local_loss.append(loss.item())\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        loss_record.append(mean_loss)\n",
    "        print(\"Epoch %i: loss=%.5f\"%(i+1,mean_loss))\n",
    "        return loss_record\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "opQOk79wrJHg",
    "outputId": "cc7fa9a7-a298-4ebc-f592-867ecc72a096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEEPTI~1\\AppData\\Local\\Temp/ipykernel_10412/1447933740.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\DEEPTI~1\\AppData\\Local\\Temp/ipykernel_10412/2579881811.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(x, y, alpha, Nepochs)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;31m#print(\"Batch Number: %i\"%(j+1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforwarddynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspikes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecords\u001b[0m \u001b[1;31m#this is used in regularization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m       \u001b[1;31m#https://pytorch.org/docs/stable/generated/torch.max.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DEEPTI~1\\AppData\\Local\\Temp/ipykernel_10412/2839323336.py\u001b[0m in \u001b[0;36mforwarddynamic\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#hidden layer 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0moutputlayer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdatatype\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ijk,kl->ijl\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \"\"\"i gives the spike train number, j and k describe the firing time and the neuron unit which fires. k and l describes the dimesnions of weight matrix. k is\n\u001b[0;32m     17\u001b[0m     the index of multiplication ==> dot product\"\"\"\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36meinsum\u001b[1;34m(equation, *operands)\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;31m# recurse incase operands contains value that has torch function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;31m# in the original implementation this line is omitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36meinsum\u001b[1;34m(equation, *operands)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_hist = training(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Zs6zaTTrJHg"
   },
   "outputs": [],
   "source": [
    "def plotloss(loss_hist):\n",
    "    plt.figure()\n",
    "    plt.plot(loss_hist, label=\"Loss\") \n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlTqvWLmrJHh"
   },
   "outputs": [],
   "source": [
    "plotloss(loss_hist)\n",
    "print(\"Trained for 10 epochs\")\n",
    "print(\"Training accuracy: %.3f\"%(accuracy(x_train,y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyT8gXyZrJHi"
   },
   "outputs": [],
   "source": [
    "Nepochs = 250\n",
    "loss_hist = training(x_train, y_train , Nepochs = Nepochs)\n",
    "plotloss(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bW_zPweRrJHm"
   },
   "outputs": [],
   "source": [
    "def accuracy(x,y):\n",
    "    acc = []\n",
    "    for x_local, y_local in sparse_data_generator_from_hdf5_spikes(X =x,y= y,batch_size = Nbatch, nb_steps = Ntimesteps, nb_units = Nin, max_time = T,shuffle=False):\n",
    "        output,_ = forwarddynamic(x_local.to_dense())\n",
    "        maximum,_ = torch.max(output,dim =1)  #calculates maximum membrane potential in the entire duration\n",
    "        _,y_unit = torch.max(maximum,1) #the unit which had maximum\n",
    "        tmp = np.mean((y_local==y_unit).detach().cpu().numpy()) # compare to labels\n",
    "        acc.append(tmp)\n",
    "    return np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpVFV7lLrJHn"
   },
   "outputs": [],
   "source": [
    "print(\"Training accuracy: %.3f\"%(accuracy(x_train,y_train)))\n",
    "print(\"Test accuracy: %.3f\"%(accuracy(x_test,y_test)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TestRecurrentSNN-SHD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
