/home/venkat/Unreliable_Synaptic_Transmission/Deepti/DOPcodes/SHD/dataset/shddataset/
init done
Epoch 1: loss=3.30265
Epoch 2: loss=2.90173
Epoch 3: loss=2.84735
Epoch 4: loss=2.78956
Epoch 5: loss=2.72669
Epoch 6: loss=2.64633
Epoch 7: loss=2.57864
Epoch 8: loss=2.50165
Epoch 9: loss=2.43133
Epoch 10: loss=2.36075
Epoch 11: loss=2.29231
Epoch 12: loss=2.23401
Epoch 13: loss=2.17280
Epoch 14: loss=2.11400
Epoch 15: loss=2.06159
Epoch 16: loss=2.01213
Epoch 17: loss=1.96620
Epoch 18: loss=1.91746
Epoch 19: loss=1.87341
Epoch 20: loss=1.83342
Epoch 21: loss=1.77994
Epoch 22: loss=1.74627
Epoch 23: loss=1.71768
Epoch 24: loss=1.67715
Epoch 25: loss=1.64516
Epoch 26: loss=1.61810
Epoch 27: loss=1.57666
Epoch 28: loss=1.55800
Epoch 29: loss=1.52441
Epoch 30: loss=1.49726
Epoch 31: loss=1.45888
Epoch 32: loss=1.41611
Epoch 33: loss=1.39101
Epoch 34: loss=1.36660
Epoch 35: loss=1.34085
Epoch 36: loss=1.31239
Epoch 37: loss=1.28400
Epoch 38: loss=1.26681
Epoch 39: loss=1.23965
Epoch 40: loss=1.22148
Epoch 41: loss=1.20080
Epoch 42: loss=1.17703
Epoch 43: loss=1.15697
Epoch 44: loss=1.14151
Epoch 45: loss=1.12098
Epoch 46: loss=1.10626
Epoch 47: loss=1.09012
Epoch 48: loss=1.07455
Epoch 49: loss=1.05328
Epoch 50: loss=1.04009
Epoch 51: loss=1.03133
Epoch 52: loss=1.01206
Epoch 53: loss=0.99463
Epoch 54: loss=0.97762
Epoch 55: loss=0.96245
Epoch 56: loss=0.95260
Epoch 57: loss=0.94054
Epoch 58: loss=0.92573
Epoch 59: loss=0.91482
Epoch 60: loss=0.89659
Epoch 61: loss=0.89101
Epoch 62: loss=0.87898
Epoch 63: loss=0.86189
Epoch 64: loss=0.85290
Epoch 65: loss=0.85590
Epoch 66: loss=0.83960
Epoch 67: loss=0.82631
Epoch 68: loss=0.81559
Epoch 69: loss=0.80493
Epoch 70: loss=0.79737
Epoch 71: loss=0.78835
Epoch 72: loss=0.78075
Epoch 73: loss=0.76706
Epoch 74: loss=0.75393
Epoch 75: loss=0.74638
Epoch 76: loss=0.73609
Epoch 77: loss=0.72645
Epoch 78: loss=0.72124
Epoch 79: loss=0.71995
Epoch 80: loss=0.71326
Epoch 81: loss=0.70381
Epoch 82: loss=0.69323
Epoch 83: loss=0.68940
Epoch 84: loss=0.67871
Epoch 85: loss=0.67666
Epoch 86: loss=0.66735
Epoch 87: loss=0.65867
Epoch 88: loss=0.66832
Epoch 89: loss=0.64722
Epoch 90: loss=0.63928
Epoch 91: loss=0.63534
Epoch 92: loss=0.63511
Epoch 93: loss=0.62435
Epoch 94: loss=0.61858
Epoch 95: loss=0.61344
Epoch 96: loss=0.60741
Epoch 97: loss=0.60131
Epoch 98: loss=0.59345
Epoch 99: loss=0.59542
Epoch 100: loss=0.58869
Epoch 101: loss=0.57904
Epoch 102: loss=0.57346
Epoch 103: loss=0.56870
Epoch 104: loss=0.55874
Epoch 105: loss=0.55036
Epoch 106: loss=0.55621
Epoch 107: loss=0.55700
Epoch 108: loss=0.53655
Epoch 109: loss=0.53748
Epoch 110: loss=0.53551
Epoch 111: loss=0.53235
Epoch 112: loss=0.52219
Epoch 113: loss=0.51920
Epoch 114: loss=0.51808
Epoch 115: loss=0.50969
Epoch 116: loss=0.50070
Epoch 117: loss=0.50468
Epoch 118: loss=0.50345
Epoch 119: loss=0.48481
Epoch 120: loss=0.47772
Epoch 121: loss=0.47831
Epoch 122: loss=0.47878
Epoch 123: loss=0.47842
Epoch 124: loss=0.48497
Epoch 125: loss=0.46868
Epoch 126: loss=0.46083
Epoch 127: loss=0.45859
Epoch 128: loss=0.45323
Epoch 129: loss=0.45165
Epoch 130: loss=0.44091
Epoch 131: loss=0.44171
Epoch 132: loss=0.44615
Epoch 133: loss=0.44299
Epoch 134: loss=0.43708
Epoch 135: loss=0.42872
Epoch 136: loss=0.42387
Epoch 137: loss=0.42918
Epoch 138: loss=0.42164
Epoch 139: loss=0.41443
Epoch 140: loss=0.41330
Epoch 141: loss=0.41210
Epoch 142: loss=0.40090
Epoch 143: loss=0.39682
Epoch 144: loss=0.40023
Epoch 145: loss=0.40440
Epoch 146: loss=0.39288
Epoch 147: loss=0.38549
Epoch 148: loss=0.38969
Epoch 149: loss=0.39317
Epoch 150: loss=0.38316
Epoch 151: loss=0.37862
Epoch 152: loss=0.37388
Epoch 153: loss=0.36699
Epoch 154: loss=0.37807
Epoch 155: loss=0.36721
Epoch 156: loss=0.36928
Epoch 157: loss=0.35827
Epoch 158: loss=0.35389
Epoch 159: loss=0.35755
Epoch 160: loss=0.35594
Epoch 161: loss=0.35180
Epoch 162: loss=0.35343
Epoch 163: loss=0.34611
Epoch 164: loss=0.33469
Epoch 165: loss=0.33445
Epoch 166: loss=0.33406
Epoch 167: loss=0.33579
Epoch 168: loss=0.32717
Epoch 169: loss=0.33088
Epoch 170: loss=0.32513
Epoch 171: loss=0.32682
Epoch 172: loss=0.32977
Epoch 173: loss=0.31782
Epoch 174: loss=0.31287
Epoch 175: loss=0.31190
Epoch 176: loss=0.30864
Epoch 177: loss=0.30572
Epoch 178: loss=0.30199
Epoch 179: loss=0.30207
Epoch 180: loss=0.30655
Epoch 181: loss=0.30095
Epoch 182: loss=0.29483
Epoch 183: loss=0.30037
Epoch 184: loss=0.29831
Epoch 185: loss=0.29749
Epoch 186: loss=0.29062
Epoch 187: loss=0.28206
Epoch 188: loss=0.28639
Epoch 189: loss=0.28592
Epoch 190: loss=0.28091
Epoch 191: loss=0.27304
Epoch 192: loss=0.27607
Epoch 193: loss=0.27921
Epoch 194: loss=0.27228
Epoch 195: loss=0.27326
Epoch 196: loss=0.27149
Epoch 197: loss=0.26712
Epoch 198: loss=0.26286
Epoch 199: loss=0.26127
Epoch 200: loss=0.26768
Epoch 201: loss=0.25953
Epoch 202: loss=0.26156
Epoch 203: loss=0.25763
Epoch 204: loss=0.26123
Epoch 205: loss=0.25545
Epoch 206: loss=0.25277
Epoch 207: loss=0.24521
Epoch 208: loss=0.24530
Epoch 209: loss=0.24199
Epoch 210: loss=0.24240
Epoch 211: loss=0.24200
Epoch 212: loss=0.24328
Epoch 213: loss=0.24753
Epoch 214: loss=0.24102
Epoch 215: loss=0.23671
Epoch 216: loss=0.23867
Epoch 217: loss=0.22984
Epoch 218: loss=0.23223
Epoch 219: loss=0.23014
Epoch 220: loss=0.23202
Epoch 221: loss=0.22855
Epoch 222: loss=0.22624
Epoch 223: loss=0.22768
Epoch 224: loss=0.22361
Epoch 225: loss=0.21918
Epoch 226: loss=0.22305
Epoch 227: loss=0.22101
Epoch 228: loss=0.21561
Epoch 229: loss=0.21547
Epoch 230: loss=0.21806
Epoch 231: loss=0.21351
Epoch 232: loss=0.21029
Epoch 233: loss=0.20834
Epoch 234: loss=0.20992
Epoch 235: loss=0.20778
Epoch 236: loss=0.20720
Epoch 237: loss=0.20283
Epoch 238: loss=0.20302
Epoch 239: loss=0.20300
Epoch 240: loss=0.19910
Epoch 241: loss=0.19914
Epoch 242: loss=0.19891
Epoch 243: loss=0.20258
Epoch 244: loss=0.19351
Epoch 245: loss=0.19420
Epoch 246: loss=0.19021
Epoch 247: loss=0.19244
Epoch 248: loss=0.19285
Epoch 249: loss=0.19368
Epoch 250: loss=0.18764
Epoch 251: loss=0.18547
Epoch 252: loss=0.18652
Epoch 253: loss=0.18211
Epoch 254: loss=0.18460
Epoch 255: loss=0.18199
Epoch 256: loss=0.17746
Epoch 257: loss=0.18150
Epoch 258: loss=0.17702
Epoch 259: loss=0.17426
Epoch 260: loss=0.17973
Epoch 261: loss=0.17699
Epoch 262: loss=0.17513
Epoch 263: loss=0.17309
Epoch 264: loss=0.17491
Epoch 265: loss=0.17433
Epoch 266: loss=0.17366
Epoch 267: loss=0.17198
Epoch 268: loss=0.17350
Epoch 269: loss=0.17022
Epoch 270: loss=0.16715
Epoch 271: loss=0.16365
Epoch 272: loss=0.16204
Epoch 273: loss=0.16490
Epoch 274: loss=0.15681
Epoch 275: loss=0.15944
Epoch 276: loss=0.16465
Epoch 277: loss=0.15990
Epoch 278: loss=0.15629
Epoch 279: loss=0.15861
Epoch 280: loss=0.15644
Epoch 281: loss=0.15391
Epoch 282: loss=0.15381
Epoch 283: loss=0.15461
Epoch 284: loss=0.15298
Epoch 285: loss=0.15041
Epoch 286: loss=0.15132
Epoch 287: loss=0.15448
Epoch 288: loss=0.15340
Epoch 289: loss=0.14907
Epoch 290: loss=0.14963
Epoch 291: loss=0.15501
Epoch 292: loss=0.14762
Epoch 293: loss=0.14545
Epoch 294: loss=0.14200
Epoch 295: loss=0.14151
Epoch 296: loss=0.14202
Epoch 297: loss=0.14259
Epoch 298: loss=0.13793
Epoch 299: loss=0.13875
Epoch 300: loss=0.13597
Epoch 301: loss=0.13558
Epoch 302: loss=0.13730
Epoch 303: loss=0.13784
Epoch 304: loss=0.13983
Epoch 305: loss=0.13674
Epoch 306: loss=0.13515
Epoch 307: loss=0.13394
Epoch 308: loss=0.13558
Epoch 309: loss=0.13551
Epoch 310: loss=0.13252
Epoch 311: loss=0.13065
Epoch 312: loss=0.12926
Epoch 313: loss=0.12799
Epoch 314: loss=0.13152
Epoch 315: loss=0.12937
Epoch 316: loss=0.12797
Epoch 317: loss=0.12852
Epoch 318: loss=0.12938
Epoch 319: loss=0.12685
Epoch 320: loss=0.12725
Epoch 321: loss=0.12439
Epoch 322: loss=0.12701
Epoch 323: loss=0.13022
Epoch 324: loss=0.12660
Epoch 325: loss=0.12541
Epoch 326: loss=0.12025
Epoch 327: loss=0.11969
Epoch 328: loss=0.12104
Epoch 329: loss=0.11855
Epoch 330: loss=0.11651
Epoch 331: loss=0.11614
Epoch 332: loss=0.11719
Epoch 333: loss=0.11537
Epoch 334: loss=0.11780
Epoch 335: loss=0.11704
Epoch 336: loss=0.11652
Epoch 337: loss=0.11606
Epoch 338: loss=0.11413
Epoch 339: loss=0.11421
Epoch 340: loss=0.11479
Epoch 341: loss=0.11237
original_recur.py:67: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  labels_ = np.array(y,dtype=np.int)
Epoch 342: loss=0.11089
Epoch 343: loss=0.11253
Epoch 344: loss=0.11146
Epoch 345: loss=0.11138
Epoch 346: loss=0.11275
Epoch 347: loss=0.10970
Epoch 348: loss=0.11005
Epoch 349: loss=0.10834
Epoch 350: loss=0.10754
Epoch 351: loss=0.10531
Epoch 352: loss=0.10625
Epoch 353: loss=0.10582
Epoch 354: loss=0.10356
Epoch 355: loss=0.10552
Epoch 356: loss=0.10045
Epoch 357: loss=0.10257
Epoch 358: loss=0.10312
Epoch 359: loss=0.10419
Epoch 360: loss=0.10418
Epoch 361: loss=0.10373
Epoch 362: loss=0.10122
Epoch 363: loss=0.10184
Epoch 364: loss=0.09769
Epoch 365: loss=0.09980
Epoch 366: loss=0.10116
Epoch 367: loss=0.10036
Epoch 368: loss=0.09728
Epoch 369: loss=0.09655
Epoch 370: loss=0.09526
Epoch 371: loss=0.09525
Epoch 372: loss=0.09904
Epoch 373: loss=0.09591
Epoch 374: loss=0.09457
Epoch 375: loss=0.09436
Epoch 376: loss=0.09407
Epoch 377: loss=0.09529
Epoch 378: loss=0.09230
Epoch 379: loss=0.09318
Epoch 380: loss=0.09634
Epoch 381: loss=0.09804
Epoch 382: loss=0.09761
Epoch 383: loss=0.09221
Epoch 384: loss=0.09291
Epoch 385: loss=0.09075
Epoch 386: loss=0.09095
Epoch 387: loss=0.08970
Epoch 388: loss=0.08997
Epoch 389: loss=0.09097
Epoch 390: loss=0.08869
Epoch 391: loss=0.08872
Epoch 392: loss=0.08722
Epoch 393: loss=0.08799
Epoch 394: loss=0.08835
Epoch 395: loss=0.08943
Epoch 396: loss=0.08787
Epoch 397: loss=0.09061
Epoch 398: loss=0.08670
Epoch 399: loss=0.08469
Epoch 400: loss=0.08347
Epoch 401: loss=0.08242
Epoch 402: loss=0.08184
Epoch 403: loss=0.08174
Epoch 404: loss=0.08328
Epoch 405: loss=0.08371
Epoch 406: loss=0.08490
Epoch 407: loss=0.08383
Epoch 408: loss=0.08468
Epoch 409: loss=0.08341
Epoch 410: loss=0.08062
Epoch 411: loss=0.08064
Epoch 412: loss=0.08274
Epoch 413: loss=0.08364
Epoch 414: loss=0.08286
Epoch 415: loss=0.08431
Epoch 416: loss=0.08214
Epoch 417: loss=0.07866
Epoch 418: loss=0.07758
Epoch 419: loss=0.07863
Epoch 420: loss=0.07764
Epoch 421: loss=0.08011
Epoch 422: loss=0.08066
Epoch 423: loss=0.07836
Epoch 424: loss=0.07707
Epoch 425: loss=0.07746
Epoch 426: loss=0.08184
Epoch 427: loss=0.07818
Epoch 428: loss=0.07519
Epoch 429: loss=0.07480
Epoch 430: loss=0.07723
Epoch 431: loss=0.07778
Epoch 432: loss=0.07686
Epoch 433: loss=0.07591
Epoch 434: loss=0.07683
Epoch 435: loss=0.07404
Epoch 436: loss=0.07462
Epoch 437: loss=0.07357
Epoch 438: loss=0.07313
Epoch 439: loss=0.07257
Epoch 440: loss=0.07396
Epoch 441: loss=0.07695
Epoch 442: loss=0.07351
Epoch 443: loss=0.07344
Epoch 444: loss=0.07381
Epoch 445: loss=0.07627
Epoch 446: loss=0.07458
Epoch 447: loss=0.07255
Epoch 448: loss=0.07237
Epoch 449: loss=0.07361
Epoch 450: loss=0.06867
Epoch 451: loss=0.06852
Epoch 452: loss=0.06867
Epoch 453: loss=0.06891
Epoch 454: loss=0.06883
Epoch 455: loss=0.07360
Epoch 456: loss=0.07045
Epoch 457: loss=0.06889
Epoch 458: loss=0.06669
Epoch 459: loss=0.06648
Epoch 460: loss=0.06683
Epoch 461: loss=0.06746
Epoch 462: loss=0.06821
Epoch 463: loss=0.06604
Epoch 464: loss=0.06821
Epoch 465: loss=0.06849
Epoch 466: loss=0.06627
Epoch 467: loss=0.06580
Epoch 468: loss=0.06714
Epoch 469: loss=0.06925
Epoch 470: loss=0.06603
Epoch 471: loss=0.06514
Epoch 472: loss=0.06564
Epoch 473: loss=0.06736
Epoch 474: loss=0.06577
Epoch 475: loss=0.06580
Epoch 476: loss=0.06738
Epoch 477: loss=0.06883
Epoch 478: loss=0.06457
Epoch 479: loss=0.06376
Epoch 480: loss=0.06217
Epoch 481: loss=0.06224
Epoch 482: loss=0.06329
Epoch 483: loss=0.06445
Epoch 484: loss=0.06284
Epoch 485: loss=0.06181
Epoch 486: loss=0.06240
Epoch 487: loss=0.06218
Epoch 488: loss=0.06103
Epoch 489: loss=0.06180
Epoch 490: loss=0.06025
Epoch 491: loss=0.06228
Epoch 492: loss=0.06056
Epoch 493: loss=0.06110
Epoch 494: loss=0.06143
Epoch 495: loss=0.06380
Epoch 496: loss=0.06201
Epoch 497: loss=0.06038
Epoch 498: loss=0.05954
Epoch 499: loss=0.06084
Epoch 500: loss=0.06073
Training accuracy: 0.999
Test accuracy: 0.645
Traceback (most recent call last):
  File "original_recur.py", line 253, in <module>
    pickle.dump(loss_list,open_file)
NameError: name 'pickle' is not defined
srun: error: gpunode1: task 0: Exited with exit code 1
