/home/venkat/Unreliable_Synaptic_Transmission/Deepti/DOPcodes/SHD/dataset/shddataset/
File not present for network with uniform weight initialisation
Initialised with numbers from uniform distribution
Epoch 1: loss=19.33665
Epoch 2: loss=3.87719
Epoch 3: loss=3.28101
Epoch 4: loss=3.02973
Epoch 5: loss=2.84375
Epoch 6: loss=2.77526
Epoch 7: loss=2.66210
Epoch 8: loss=2.57116
Epoch 9: loss=2.43032
Epoch 10: loss=2.38716
Trained for 10 epochs
Training accuracy: 0.418
Epoch 1: loss=2.25706
Epoch 2: loss=2.16564
Epoch 3: loss=2.12638
Epoch 4: loss=2.01161
Epoch 5: loss=1.96214
Epoch 6: loss=1.92662
Epoch 7: loss=1.81705
Epoch 8: loss=1.80988
Epoch 9: loss=1.73648
Epoch 10: loss=1.71181
Epoch 11: loss=1.65098
Epoch 12: loss=1.61917
Epoch 13: loss=1.54853
Epoch 14: loss=1.52863
Epoch 15: loss=1.47756
Epoch 16: loss=1.43496
Epoch 17: loss=1.44239
Epoch 18: loss=1.39558
Epoch 19: loss=1.36882
Epoch 20: loss=1.31062
Epoch 21: loss=1.27497
Epoch 22: loss=1.27654
Epoch 23: loss=1.24405
Epoch 24: loss=1.19784
Epoch 25: loss=1.16850
Epoch 26: loss=1.17800
Epoch 27: loss=1.11490
Epoch 28: loss=1.09871
Epoch 29: loss=1.09795
Epoch 30: loss=1.06477
Epoch 31: loss=1.05294
Epoch 32: loss=1.04090
Epoch 33: loss=1.04696
Epoch 34: loss=1.02279
Epoch 35: loss=0.99586
Epoch 36: loss=0.96503
Epoch 37: loss=0.95258
Epoch 38: loss=0.96207
Epoch 39: loss=0.92873
Epoch 40: loss=0.90894
Epoch 41: loss=0.91156
Epoch 42: loss=0.88611
Epoch 43: loss=0.85759
Epoch 44: loss=0.86221
Epoch 45: loss=0.85744
Epoch 46: loss=0.83382
Epoch 47: loss=0.82342
Epoch 48: loss=0.80874
Epoch 49: loss=0.80830
Epoch 50: loss=0.81544
Epoch 51: loss=0.79429
Epoch 52: loss=0.77946
Epoch 53: loss=0.76163
Epoch 54: loss=0.74737
Epoch 55: loss=0.73275
Epoch 56: loss=0.74773
Epoch 57: loss=0.73256
Epoch 58: loss=0.75051
Epoch 59: loss=0.73002
Epoch 60: loss=0.69335
Epoch 61: loss=0.68828
Epoch 62: loss=0.67762
Epoch 63: loss=0.67130
Epoch 64: loss=0.68487
Epoch 65: loss=0.68132
Epoch 66: loss=0.65956
Epoch 67: loss=0.64954
Epoch 68: loss=0.65029
Epoch 69: loss=0.66318
Epoch 70: loss=0.66277
Epoch 71: loss=0.62464
Epoch 72: loss=0.63255
Epoch 73: loss=0.63814
Epoch 74: loss=0.61219
Epoch 75: loss=0.60537
Epoch 76: loss=0.61912
Epoch 77: loss=0.59788
Epoch 78: loss=0.60187
Epoch 79: loss=0.61014
Epoch 80: loss=0.60349
Epoch 81: loss=0.57347
Epoch 82: loss=0.57910
Epoch 83: loss=0.58810
Epoch 84: loss=0.57266
Epoch 85: loss=0.56275
Epoch 86: loss=0.56254
Epoch 87: loss=0.54686
Epoch 88: loss=0.60070
Epoch 89: loss=0.56591
Epoch 90: loss=0.53794
Epoch 91: loss=0.56575
Epoch 92: loss=0.53236
Epoch 93: loss=0.53678
Epoch 94: loss=0.51335
Epoch 95: loss=0.52606
Epoch 96: loss=0.51948
Epoch 97: loss=0.49682
Epoch 98: loss=0.51126
Epoch 99: loss=0.50664
Epoch 100: loss=0.51471
Epoch 101: loss=0.49380
Epoch 102: loss=0.49270
Epoch 103: loss=0.49936
Epoch 104: loss=0.49804
Epoch 105: loss=0.49052
Epoch 106: loss=0.51517
Epoch 107: loss=0.49851
Epoch 108: loss=0.46991
Epoch 109: loss=0.46545
Epoch 110: loss=0.47565
Epoch 111: loss=0.44659
Epoch 112: loss=0.44926
Epoch 113: loss=0.43058
Epoch 114: loss=0.45600
Epoch 115: loss=0.45779
Epoch 116: loss=0.45294
Epoch 117: loss=0.44469
Epoch 118: loss=0.42165
Epoch 119: loss=0.44357
Epoch 120: loss=0.45527
Epoch 121: loss=0.45584
Epoch 122: loss=0.43647
Epoch 123: loss=0.45945
Epoch 124: loss=0.45535
Epoch 125: loss=0.42150
Epoch 126: loss=0.42194
Epoch 127: loss=0.43956
Epoch 128: loss=0.42470
Epoch 129: loss=0.42939
Epoch 130: loss=0.43582
Epoch 131: loss=0.42099
Epoch 132: loss=0.41709
Epoch 133: loss=0.40377
Epoch 134: loss=0.40534
Epoch 135: loss=0.38172
Epoch 136: loss=0.38960
Epoch 137: loss=0.39427
Epoch 138: loss=0.40781
Epoch 139: loss=0.38602
Epoch 140: loss=0.37303
Epoch 141: loss=0.37747
Epoch 142: loss=0.39611
Epoch 143: loss=0.37722
Epoch 144: loss=0.38976
Epoch 145: loss=0.37828
Epoch 146: loss=0.37135
Epoch 147: loss=0.35695
Epoch 148: loss=0.35560
Epoch 149: loss=0.38188
Epoch 150: loss=0.38716
Epoch 151: loss=0.37615
Epoch 152: loss=0.34993
Epoch 153: loss=0.38498
Epoch 154: loss=0.37444
Epoch 155: loss=0.34500
Epoch 156: loss=0.35422
Epoch 157: loss=0.34617
Epoch 158: loss=0.34705
Epoch 159: loss=0.33303
Epoch 160: loss=0.34416
Epoch 161: loss=0.34063
Epoch 162: loss=0.35537
Epoch 163: loss=0.37716
Epoch 164: loss=0.32318
Epoch 165: loss=0.32983
Epoch 166: loss=0.34317
Epoch 167: loss=0.33174
Epoch 168: loss=0.32483
Epoch 169: loss=0.34092
Epoch 170: loss=0.33083
Epoch 171: loss=0.33062
Epoch 172: loss=0.30773
Epoch 173: loss=0.31553
Epoch 174: loss=0.34050
Epoch 175: loss=0.32133
Epoch 176: loss=0.32444
Epoch 177: loss=0.33393
Epoch 178: loss=0.32173
Epoch 179: loss=0.32220
Epoch 180: loss=0.34199
Epoch 181: loss=0.31721
Epoch 182: loss=0.31210
Epoch 183: loss=0.32424
Epoch 184: loss=0.31075
Epoch 185: loss=0.30301
Epoch 186: loss=0.31799
Epoch 187: loss=0.31554
Epoch 188: loss=0.31629
Epoch 189: loss=0.30490
Epoch 190: loss=0.30302
Epoch 191: loss=0.30354
Epoch 192: loss=0.30320
Epoch 193: loss=0.31006
Epoch 194: loss=0.32149
Epoch 195: loss=0.30213
Epoch 196: loss=0.30600
Epoch 197: loss=0.29319
Epoch 198: loss=0.29702
Epoch 199: loss=0.29843
Epoch 200: loss=0.28126
Training accuracy: 0.957
Test accuracy: 0.580

File list dumped

Saved in file
500 epochs recur round 2
are new values in file?
