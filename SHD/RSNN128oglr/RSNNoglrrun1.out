/home/venkat/Unreliable_Synaptic_Transmission/Deepti/DOPcodes/SHD/dataset/shddataset/
File not present for network with uniform weight initialisation
Initialised with numbers from uniform distribution
Epoch 1: loss=16.83047
Epoch 2: loss=3.68160
Epoch 3: loss=3.31783
Epoch 4: loss=3.03265
Epoch 5: loss=2.99784
Epoch 6: loss=2.90472
Epoch 7: loss=2.75476
Epoch 8: loss=2.63107
Epoch 9: loss=2.52012
Epoch 10: loss=2.45456
Trained for 10 epochs
Training accuracy: 0.335
Epoch 1: loss=2.36370
Epoch 2: loss=2.31981
Epoch 3: loss=2.29081
Epoch 4: loss=2.20486
Epoch 5: loss=2.17298
Epoch 6: loss=2.12167
Epoch 7: loss=2.06503
Epoch 8: loss=2.03107
Epoch 9: loss=2.01083
Epoch 10: loss=1.94672
Epoch 11: loss=1.90646
Epoch 12: loss=1.83795
Epoch 13: loss=1.81857
Epoch 14: loss=1.77856
Epoch 15: loss=1.77587
Epoch 16: loss=1.76055
Epoch 17: loss=1.74843
Epoch 18: loss=1.65771
Epoch 19: loss=1.63940
Epoch 20: loss=1.64678
Epoch 21: loss=1.55958
Epoch 22: loss=1.53836
Epoch 23: loss=1.47718
Epoch 24: loss=1.45630
Epoch 25: loss=1.46034
Epoch 26: loss=1.41479
Epoch 27: loss=1.41537
Epoch 28: loss=1.37219
Epoch 29: loss=1.42825
Epoch 30: loss=1.37995
Epoch 31: loss=1.36813
Epoch 32: loss=1.30900
Epoch 33: loss=1.28934
Epoch 34: loss=1.24473
Epoch 35: loss=1.24177
Epoch 36: loss=1.23537
Epoch 37: loss=1.19716
Epoch 38: loss=1.18310
Epoch 39: loss=1.15635
Epoch 40: loss=1.15928
Epoch 41: loss=1.13502
Epoch 42: loss=1.10012
Epoch 43: loss=1.09901
Epoch 44: loss=1.09015
Epoch 45: loss=1.10246
Epoch 46: loss=1.08340
Epoch 47: loss=1.07876
Epoch 48: loss=1.05323
Epoch 49: loss=1.05705
Epoch 50: loss=1.03832
Epoch 51: loss=1.01307
Epoch 52: loss=1.00800
Epoch 53: loss=0.98296
Epoch 54: loss=0.98948
Epoch 55: loss=0.98304
Epoch 56: loss=0.95574
Epoch 57: loss=0.95949
Epoch 58: loss=0.94462
Epoch 59: loss=0.90249
Epoch 60: loss=0.93860
Epoch 61: loss=0.91877
Epoch 62: loss=0.90469
Epoch 63: loss=0.88719
Epoch 64: loss=0.89306
Epoch 65: loss=0.84565
Epoch 66: loss=0.85639
Epoch 67: loss=0.86371
Epoch 68: loss=0.84291
Epoch 69: loss=0.83355
Epoch 70: loss=0.80530
Epoch 71: loss=0.83927
Epoch 72: loss=0.82858
Epoch 73: loss=0.77501
Epoch 74: loss=0.77721
Epoch 75: loss=0.76877
Epoch 76: loss=0.76956
Epoch 77: loss=0.74301
Epoch 78: loss=0.73279
Epoch 79: loss=0.75992
Epoch 80: loss=0.72769
Epoch 81: loss=0.74036
Epoch 82: loss=0.72536
Epoch 83: loss=0.72649
Epoch 84: loss=0.72388
Epoch 85: loss=0.71064
Epoch 86: loss=0.69540
Epoch 87: loss=0.69713
Epoch 88: loss=0.70268
Epoch 89: loss=0.71108
Epoch 90: loss=0.68794
Epoch 91: loss=0.67988
Epoch 92: loss=0.67881
Epoch 93: loss=0.65342
Epoch 94: loss=0.67146
Epoch 95: loss=0.66382
Epoch 96: loss=0.67946
Epoch 97: loss=0.71024
Epoch 98: loss=0.67045
Epoch 99: loss=0.64958
Epoch 100: loss=0.64284
Epoch 101: loss=0.66716
Epoch 102: loss=0.65229
Epoch 103: loss=0.65588
Epoch 104: loss=0.62723
Epoch 105: loss=0.60917
Epoch 106: loss=0.61268
Epoch 107: loss=0.60374
Epoch 108: loss=0.62158
Epoch 109: loss=0.59511
Epoch 110: loss=0.61018
Epoch 111: loss=0.60270
Epoch 112: loss=0.61045
Epoch 113: loss=0.57025
Epoch 114: loss=0.59197
Epoch 115: loss=0.57408
Epoch 116: loss=0.59747
Epoch 117: loss=0.58072
Epoch 118: loss=0.57610
Epoch 119: loss=0.55530
Epoch 120: loss=0.57000
Epoch 121: loss=0.56499
Epoch 122: loss=0.55763
Epoch 123: loss=0.56041
Epoch 124: loss=0.55849
Epoch 125: loss=0.54867
Epoch 126: loss=0.56368
Epoch 127: loss=0.55467
Epoch 128: loss=0.52235
Epoch 129: loss=0.52620
Epoch 130: loss=0.51827
Epoch 131: loss=0.54470
Epoch 132: loss=0.52886
Epoch 133: loss=0.52597
Epoch 134: loss=0.52853
Epoch 135: loss=0.53080
Epoch 136: loss=0.51219
Epoch 137: loss=0.50834
Epoch 138: loss=0.54333
Epoch 139: loss=0.51171
Epoch 140: loss=0.49676
Epoch 141: loss=0.52436
Epoch 142: loss=0.50545
Epoch 143: loss=0.50949
Epoch 144: loss=0.50940
Epoch 145: loss=0.50395
Epoch 146: loss=0.49382
Epoch 147: loss=0.52105
Epoch 148: loss=0.50845
Epoch 149: loss=0.51777
Epoch 150: loss=0.50533
Epoch 151: loss=0.50172
Epoch 152: loss=0.49619
Epoch 153: loss=0.47492
Epoch 154: loss=0.47439
Epoch 155: loss=0.50274
Epoch 156: loss=0.49613
Epoch 157: loss=0.51651
Epoch 158: loss=0.47992
Epoch 159: loss=0.49167
Epoch 160: loss=0.49973
Epoch 161: loss=0.48090
Epoch 162: loss=0.46483
Epoch 163: loss=0.48308
Epoch 164: loss=0.46811
Epoch 165: loss=0.48071
Epoch 166: loss=0.45758
Epoch 167: loss=0.45755
Epoch 168: loss=0.45452
Epoch 169: loss=0.47033
Epoch 170: loss=0.44373
Epoch 171: loss=0.42800
Epoch 172: loss=0.43685
Epoch 173: loss=0.44880
Epoch 174: loss=0.45003
Epoch 175: loss=0.44976
Epoch 176: loss=0.42848
Epoch 177: loss=0.43595
Epoch 178: loss=0.42719
Epoch 179: loss=0.41443
Epoch 180: loss=0.41687
Epoch 181: loss=0.42126
Epoch 182: loss=0.42887
Epoch 183: loss=0.42430
Epoch 184: loss=0.42787
Epoch 185: loss=0.41616
Epoch 186: loss=0.39038
Epoch 187: loss=0.41470
Epoch 188: loss=0.41159
Epoch 189: loss=0.40078
Epoch 190: loss=0.39493
Epoch 191: loss=0.39386
Epoch 192: loss=0.38920
Epoch 193: loss=0.40674
Epoch 194: loss=0.39238
Epoch 195: loss=0.42798
Epoch 196: loss=0.41431
Epoch 197: loss=0.40018
Epoch 198: loss=0.40288
Epoch 199: loss=0.38644
Epoch 200: loss=0.38115
Training accuracy: 0.919
Test accuracy: 0.575

File list dumped

Saved in file
500 epochs recur round 2
are new values in file?
