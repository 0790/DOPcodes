/home/venkat/Unreliable_Synaptic_Transmission/Deepti/DOPcodes/SHD/dataset/shddataset/
File not present for network with uniform weight initialisation
Initialised with numbers from uniform distribution
Epoch 1: loss=17.48557
Epoch 2: loss=3.29273
Epoch 3: loss=2.97404
Epoch 4: loss=2.83242
Epoch 5: loss=2.70912
Epoch 6: loss=2.67052
Epoch 7: loss=2.61178
Epoch 8: loss=2.54551
Epoch 9: loss=2.46835
Epoch 10: loss=2.34613
Trained for 10 epochs
Training accuracy: 0.337
Epoch 1: loss=2.28576
Epoch 2: loss=2.20515
Epoch 3: loss=2.14431
Epoch 4: loss=2.08405
Epoch 5: loss=2.04001
Epoch 6: loss=2.03689
Epoch 7: loss=1.99465
Epoch 8: loss=1.93528
Epoch 9: loss=1.90383
Epoch 10: loss=1.86369
Epoch 11: loss=1.79048
Epoch 12: loss=1.80524
Epoch 13: loss=1.75523
Epoch 14: loss=1.69738
Epoch 15: loss=1.71401
Epoch 16: loss=1.64095
Epoch 17: loss=1.61987
Epoch 18: loss=1.59770
Epoch 19: loss=1.55590
Epoch 20: loss=1.51604
Epoch 21: loss=1.48342
Epoch 22: loss=1.47953
Epoch 23: loss=1.43708
Epoch 24: loss=1.41029
Epoch 25: loss=1.40267
Epoch 26: loss=1.37187
Epoch 27: loss=1.35711
Epoch 28: loss=1.35956
Epoch 29: loss=1.31779
Epoch 30: loss=1.30943
Epoch 31: loss=1.26401
Epoch 32: loss=1.25180
Epoch 33: loss=1.27413
Epoch 34: loss=1.22840
Epoch 35: loss=1.20049
Epoch 36: loss=1.18426
Epoch 37: loss=1.16975
Epoch 38: loss=1.15988
Epoch 39: loss=1.13862
Epoch 40: loss=1.11789
Epoch 41: loss=1.14099
Epoch 42: loss=1.08043
Epoch 43: loss=1.06601
Epoch 44: loss=1.07375
Epoch 45: loss=1.05476
Epoch 46: loss=1.03860
Epoch 47: loss=1.04189
Epoch 48: loss=1.04360
Epoch 49: loss=1.00353
Epoch 50: loss=0.99694
Epoch 51: loss=0.98590
Epoch 52: loss=0.97835
Epoch 53: loss=0.96059
Epoch 54: loss=0.94165
Epoch 55: loss=0.91745
Epoch 56: loss=0.90914
Epoch 57: loss=0.92651
Epoch 58: loss=0.90605
Epoch 59: loss=0.89592
Epoch 60: loss=0.91448
Epoch 61: loss=0.87522
Epoch 62: loss=0.88900
Epoch 63: loss=0.89494
Epoch 64: loss=0.88906
Epoch 65: loss=0.86779
Epoch 66: loss=0.86708
Epoch 67: loss=0.82357
Epoch 68: loss=0.81842
Epoch 69: loss=0.83471
Epoch 70: loss=0.82659
Epoch 71: loss=0.79891
Epoch 72: loss=0.79845
Epoch 73: loss=0.81832
Epoch 74: loss=0.80668
Epoch 75: loss=0.79675
Epoch 76: loss=0.77087
Epoch 77: loss=0.75445
Epoch 78: loss=0.77160
Epoch 79: loss=0.76959
Epoch 80: loss=0.76880
Epoch 81: loss=0.75918
Epoch 82: loss=0.75600
Epoch 83: loss=0.74411
Epoch 84: loss=0.74811
Epoch 85: loss=0.73241
Epoch 86: loss=0.72434
Epoch 87: loss=0.70913
Epoch 88: loss=0.69487
Epoch 89: loss=0.71249
Epoch 90: loss=0.73629
Epoch 91: loss=0.71217
Epoch 92: loss=0.69837
Epoch 93: loss=0.70777
Epoch 94: loss=0.71222
Epoch 95: loss=0.68956
Epoch 96: loss=0.68014
Epoch 97: loss=0.67223
Epoch 98: loss=0.67163
Epoch 99: loss=0.69456
Epoch 100: loss=0.68625
Epoch 101: loss=0.65477
Epoch 102: loss=0.66567
Epoch 103: loss=0.64776
Epoch 104: loss=0.62980
Epoch 105: loss=0.64602
Epoch 106: loss=0.68250
Epoch 107: loss=0.64681
Epoch 108: loss=0.64898
Epoch 109: loss=0.64934
Epoch 110: loss=0.64086
Epoch 111: loss=0.61939
Epoch 112: loss=0.61558
Epoch 113: loss=0.62226
Epoch 114: loss=0.63287
Epoch 115: loss=0.60646
Epoch 116: loss=0.59844
Epoch 117: loss=0.61275
Epoch 118: loss=0.60493
Epoch 119: loss=0.60319
Epoch 120: loss=0.57384
Epoch 121: loss=0.60667
Epoch 122: loss=0.57976
Epoch 123: loss=0.59197
Epoch 124: loss=0.56891
Epoch 125: loss=0.58390
Epoch 126: loss=0.56164
Epoch 127: loss=0.58466
Epoch 128: loss=0.60556
Epoch 129: loss=0.59458
Epoch 130: loss=0.57134
Epoch 131: loss=0.56991
Epoch 132: loss=0.56917
Epoch 133: loss=0.57673
Epoch 134: loss=0.59734
Epoch 135: loss=0.56636
Epoch 136: loss=0.58069
Epoch 137: loss=0.57404
Epoch 138: loss=0.57627
Epoch 139: loss=0.54194
Epoch 140: loss=0.55392
Epoch 141: loss=0.56093
Epoch 142: loss=0.57765
Epoch 143: loss=0.53888
Epoch 144: loss=0.54121
Epoch 145: loss=0.54784
Epoch 146: loss=0.55933
Epoch 147: loss=0.57275
Epoch 148: loss=0.55524
Epoch 149: loss=0.52555
Epoch 150: loss=0.54595
Epoch 151: loss=0.53988
Epoch 152: loss=0.53018
Epoch 153: loss=0.54239
Epoch 154: loss=0.52663
Epoch 155: loss=0.51355
Epoch 156: loss=0.50077
Epoch 157: loss=0.51076
Epoch 158: loss=0.50763
Epoch 159: loss=0.52324
Epoch 160: loss=0.49082
Epoch 161: loss=0.50971
Epoch 162: loss=0.48871
Epoch 163: loss=0.49830
Epoch 164: loss=0.50525
Epoch 165: loss=0.51547
Epoch 166: loss=0.50173
Epoch 167: loss=0.50483
Epoch 168: loss=0.46517
Epoch 169: loss=0.49924
Epoch 170: loss=0.49109
Epoch 171: loss=0.49186
Epoch 172: loss=0.47702
Epoch 173: loss=0.47781
Epoch 174: loss=0.47011
Epoch 175: loss=0.48524
Epoch 176: loss=0.46175
Epoch 177: loss=0.46019
Epoch 178: loss=0.44973
Epoch 179: loss=0.47739
Epoch 180: loss=0.46005
Epoch 181: loss=0.46254
Epoch 182: loss=0.43114
Epoch 183: loss=0.47059
Epoch 184: loss=0.43990
Epoch 185: loss=0.46410
Epoch 186: loss=0.45820
Epoch 187: loss=0.45761
Epoch 188: loss=0.46075
Epoch 189: loss=0.45153
Epoch 190: loss=0.42040
Epoch 191: loss=0.44755
Epoch 192: loss=0.47456
Epoch 193: loss=0.46244
Epoch 194: loss=0.42876
Epoch 195: loss=0.45382
Epoch 196: loss=0.45342
Epoch 197: loss=0.47409
Epoch 198: loss=0.45469
Epoch 199: loss=0.45188
Epoch 200: loss=0.43812
Training accuracy: 0.911
Test accuracy: 0.585

File list dumped

Saved in file
500 epochs recur round 2
are new values in file?
