testff128_dropweights.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  d1 = torch.tensor(ber1.sample(w1size)).to(device)
testff128_dropweights.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  d2 = torch.tensor(ber2.sample(w2size)).to(device)
testff128_dropweights.py:270: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  dropw1 = torch.tensor(dropw1 ,requires_grad = True,dtype = datatype ).to(device)
testff128_dropweights.py:272: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  dropw2 = torch.tensor(dropw2 ,requires_grad = True,dtype = datatype ).to(device)
/home/venkat/Unreliable_Synaptic_Transmission/Deepti/DOPcodes/SHD_dropout/dataset/shddataset/
made weight variables and loss histogram
Epoch 1: loss=8.70839
Epoch 2: loss=9.24660
Epoch 3: loss=8.09293
Epoch 4: loss=6.98708
Epoch 5: loss=9.38659
Epoch 6: loss=7.79893
Epoch 7: loss=8.51352
Epoch 8: loss=7.38622
Epoch 9: loss=7.83493
Epoch 10: loss=8.08066
Trained for 10 epochs
Training accuracy: 0.058
Epoch 1: loss=3.75866
Epoch 2: loss=3.77033
Epoch 3: loss=3.69670
Epoch 4: loss=3.77036
Epoch 5: loss=3.60024
Epoch 6: loss=3.78715
Epoch 7: loss=3.81688
Epoch 8: loss=3.90030
Epoch 9: loss=3.69671
Epoch 10: loss=3.78090
Epoch 11: loss=3.90874
Epoch 12: loss=3.31628
Epoch 13: loss=4.08390
Epoch 14: loss=3.95261
Epoch 15: loss=3.75642
Epoch 16: loss=3.87836
Epoch 17: loss=3.68177
Epoch 18: loss=3.76732
Epoch 19: loss=3.67742
Epoch 20: loss=3.43463
Epoch 21: loss=3.85319
Epoch 22: loss=3.69621
Epoch 23: loss=3.72891
Epoch 24: loss=3.77394
Epoch 25: loss=3.60615
Epoch 26: loss=3.65531
Epoch 27: loss=3.75529
Epoch 28: loss=3.65861
Epoch 29: loss=3.74337
Epoch 30: loss=3.70258
Epoch 31: loss=3.65296
Epoch 32: loss=3.82584
Epoch 33: loss=3.86113
Epoch 34: loss=3.67751
Epoch 35: loss=3.95199
Epoch 36: loss=3.86929
Epoch 37: loss=3.72237
Epoch 38: loss=3.63242
Epoch 39: loss=3.62349
Epoch 40: loss=4.01166
Epoch 41: loss=3.64057
Epoch 42: loss=4.03331
Epoch 43: loss=4.24992
Epoch 44: loss=4.26434
Epoch 45: loss=3.73074
Epoch 46: loss=3.92224
Epoch 47: loss=3.84058
Epoch 48: loss=3.64072
Epoch 49: loss=3.66244
Epoch 50: loss=3.92977
Epoch 51: loss=3.52966
Epoch 52: loss=3.55190
Epoch 53: loss=3.93295
Epoch 54: loss=3.79656
Epoch 55: loss=3.86220
Epoch 56: loss=3.72828
Epoch 57: loss=3.69210
Epoch 58: loss=3.91154
Epoch 59: loss=3.52962
Epoch 60: loss=3.72321
Epoch 61: loss=3.91343
Epoch 62: loss=3.97806
Epoch 63: loss=3.85606
Epoch 64: loss=3.60865
Epoch 65: loss=3.65010
Epoch 66: loss=3.66864
Epoch 67: loss=3.82251
Epoch 68: loss=3.59737
Epoch 69: loss=3.60124
Epoch 70: loss=3.60838
Epoch 71: loss=3.92783
Epoch 72: loss=3.70292
Epoch 73: loss=3.89999
Epoch 74: loss=3.35511
Epoch 75: loss=3.86552
Epoch 76: loss=3.87847
Epoch 77: loss=3.54880
Epoch 78: loss=3.78258
Epoch 79: loss=3.62069
Epoch 80: loss=3.45966
Epoch 81: loss=3.59700
Epoch 82: loss=3.59332
Epoch 83: loss=3.91799
Epoch 84: loss=3.81703
Epoch 85: loss=3.81620
Epoch 86: loss=3.73732
Epoch 87: loss=3.91479
Epoch 88: loss=3.78438
Epoch 89: loss=3.33556
Epoch 90: loss=3.62057
Epoch 91: loss=3.88332
Epoch 92: loss=3.83214
Epoch 93: loss=3.70585
Epoch 94: loss=3.79861
Epoch 95: loss=3.51602
Epoch 96: loss=3.62897
Epoch 97: loss=3.58827
Epoch 98: loss=3.80896
Epoch 99: loss=4.19964
Epoch 100: loss=3.74385
Training accuracy: 0.058
Test accuracy: 0.049

File list dumped

Saved in file
